{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pylab\n",
    "import scipy.stats as ss\n",
    "import csv\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import glob\n",
    "from io import StringIO\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import datasets\n",
    "from sklearn import svm\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Análisis:\n",
    "\n",
    "**Carlos, Por favor, intenta escribir en estas celdas markdown, como si estuvieses escribiendo la memoria del TFG. De otra forma, llegará un momento en que no te acuerdes de lo que estás queriendo hacer. Es fundamental que vayas documentando todo lo que haces. Siento ser tan insistente sobre esto, pero es que es fundamental para poder terminar con un buen trabajo.**\n",
    "\n",
    "Básicamente tienes que se capaz de contar lo que vamos a realizar. No es trabajo en vano, porque luego lo tienes que hacer en la memoria del TFG.\n",
    "\n",
    "Todo lo que hicimos anteriormente para generar los conjuntos de entrenamiento y de test también deberías ir documentándolo, si no va a ser imposible que sólo revisando el código de acuerdes de todo y vas a perder mucho tiempo intentando entender que hacían las líneas de código."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cargamos las matrices anteriormente creadas y que fueron divididas en train y test. Cargamos tanto los pacientes con sus variables y revisiones como las etiquetas asociadas a cada paciente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.load('Conjunto_train.npy')\n",
    "X_test = np.load('Conjunto_test.npy')\n",
    "y_train = np.load('Labels_train.npy')\n",
    "y_test = np.load('Labels_test.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comprobamos las dimensiones de cada matriz, tenemos 1317 pacientes para train y 330 para test (partición 80/20).\n",
    "\n",
    "El 13 hace referencia al número de revisiones que nosotros mismos decicimos como idóneas. 22 son el número de features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_train.shape)\n",
    "print(X_train.shape)\n",
    "print(y_test.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1º Comenzaremos con dos instantes temporales (nos será más fácil trabajar así, será extrapolable a cuando tengamos un mayor número de revisiones):\n",
    "\n",
    "X_train_1[:,0,:] hace referencia al primer instante temporal (1º revisión), cogemos todos los pacientes y todas las features para ese primer instante.\n",
    "\n",
    "X_train_1[:,1,:] hace referencia al segundo instante temporal (2º revisión), cogemos todos los pacientes y todas las features para ese segundo instante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_1 = X_train[:,0,:]\n",
    "X_train_2 = X_train[:,1,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como observamos el el histograma, los datos estás desbalanceados por lo que debemos aplicar oversampling (aumentando la clase minoritaria)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(y_train)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No entiendo lo que haces aquí?? (en la celda que está debajo)\n",
    "\n",
    "Hay que tratar los datos que tenemos a NaN, pero no podemos ponerlos símplemente a 0.\n",
    "\n",
    "habría que hacer una función que nos permitiese tratar los NaNs. Vamos a hacer lo siguiente en la función:\n",
    "\n",
    "1. Revisar si hay alguna característica que tenga más del 50% a NaN, entonces habría que considerar eliminarla. Tenemos un problema, porque hay que revisar que tengo más de 50% en todas las revisiones. Para no complicar las cosas, vamos a considerar que trabajamos sólo con tres revisiones (vamos a ver si llegamos rápido a ese punto, en otro caso deberíamos ver si paramos en tres revisiones, que ya es bastante).\n",
    "\n",
    "1. Revisar si hay algún paciente con más de 8 características NaN, con que esto ocurra en cualquiera de las tres revisiones, deberíamos considerarlo que se elimina, en todas las revisiones.\n",
    "\n",
    "## Features con problemas:\n",
    "\n",
    "Revisando la información rápidamente hay varias features con problemas:\n",
    "\n",
    "1. **Importante**: las variables *Blood_Glucose* y *Glycated-HB* hay que sacarlas fuera de la base de datos, porque son las que se utilizan para determinar si un pacientes diabético o no.\n",
    "1. Hay cuatro variables que tiene muchos NaN, así que lo que intentaría sería centrarnos en cómo imputarlas, tanto en training como en test. Esto es un poco más complicado de lo que hacemos habitualmente con las bases de datos normales, porque aquí tenemos más datos y de forma un poco más complicada.\n",
    "    * HOMA\n",
    "    * Insulin\n",
    "    * Vitamin-D\n",
    "   \n",
    "Estas son las que más tienen en las tres primeras revisiones. Lo que vamos a hacer es lo siguiente, aprovechando que tenemos un montón de datos y datos temporales vamos a utilizar un esquema diferente para hacer la imputación de forma que sea más razonable, intentando perder el menor número de datos posibles, por ejemplo: \n",
    " * Regresión lineal múltiple con respecto a todas las variables temporales. Otra cosa\n",
    " * Verificar cuál es la correlación entre variables, para una misma revisión, y buscar con las que mayor correlación tengan y proponer un modelo de regresión lineal. \n",
    " \n",
    "Mi propuesta para el TFG: \n",
    " * Vamos a hacer imputación por la mediana (en el caso de Vitamin-D, HOMA e Insulin) casi seguro que vamos a empeorar un poco porque hay muchos NaN, pero la esperanza es que todo se compense.\n",
    " * Si te ves con fuerzas y ganas probamos lo que he comentado anteriormente\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = ['Age','Weight','Size','IMC','Creatinine','Cystatin','HDL','LDL','Triglyciredes','GOT','GPT','GGT','Albuminuria','Ferritin','HOMA','Insulin','Blood_Glucose','Glycated-HB','PCR','Vitamin-D','TAS','TAD','Date']\n",
    "    \n",
    "df = pd.DataFrame(X_train_1,columns = feature_names[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index,value in enumerate (df.isnull().sum()):\n",
    "        if value !=0:\n",
    "            print(df.columns[index],value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_nan_review_analysis(X):\n",
    "    \"\"\"\n",
    "    Function that takes as input a matrix with pats and features for one review, and gets back\n",
    "    X_train_rev_imputed,pats_to_drop,feature_to_drop,imputation_data\n",
    "    \"\"\"\n",
    "    feature_names = ['Age','Weight','Size','IMC','Creatinine','Cystatin','HDL','LDL','Triglyciredes','GOT','GPT','GGT','Albuminuria','Ferritin','HOMA','Insulin','Blood_Glucose','Glycated-HB','PCR','Vitamin-D','TAS','TAD','Date']\n",
    "    \n",
    "    df = pd.DataFrame(X,columns = feature_names[:-1]) #convert it to df #delete date [:-1]\n",
    "    \n",
    "    print(df.columns)\n",
    "    df = df.drop('Blood_Glucose',axis = 1)\n",
    "    df = df.drop('Glycated-HB',axis = 1)\n",
    "    \n",
    "    \n",
    "    #initialize vars to return\n",
    "    X_train_rev_imputed = []\n",
    "    pats_to_drop = []\n",
    "    feature_to_drop = []\n",
    "    imputation_data = []\n",
    "    #check number of NaNs per feature\n",
    "    \n",
    "    for index,value in enumerate (df.isnull().sum()):\n",
    "        if value !=0:\n",
    "            print(df.columns[index],value)\n",
    "            \n",
    "    #Drop features: Blood_Glucose and Glycated_HB\n",
    "    \n",
    "\n",
    "    ##reemplazar cada valor por la mediana \n",
    "    \n",
    "    X_train_rev_imputed = df.fillna(df.median()) \n",
    "    \n",
    "    return X_train_rev_imputed,pats_to_drop,feature_to_drop,imputation_data\n",
    "\n",
    "def imputing_data(X):\n",
    "    \"\"\" \n",
    "    Function that check data with nans and replace them with appropriate imputation\n",
    "    \"\"\"\n",
    "    X_train_aux = X_train.copy()\n",
    "    num_review = X_train_aux.shape[1]\n",
    "    \n",
    "    X_train_imp = []\n",
    "    \n",
    "    #run over the reviews and get the X_train_imputed, pats_to_drop, feature_to_drop, and imputation values\n",
    "    for i in range(3):\n",
    "        print(\"review\",i)\n",
    "        print(\"----------\")\n",
    "        X_train_rev_imputed,pats_to_drop,feature_to_drop,imputation_data = data_nan_review_analysis(X_train_aux[:,i,:])\n",
    "        print(\"----------\")\n",
    "        print(\"----------\")\n",
    "        \n",
    "        X_train_imp.append(X_train_rev_imputed)\n",
    "        \n",
    "    return X_train_imp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos una nueva variable: **X_train_imp**, obtenida de aplicar la función imputing_data a X_train. Devuelve lo siguiente\n",
    "\n",
    "- Nº de revisión\n",
    "- Features\n",
    "- Features con sus respectivos NaN (no aparecen las features eliminadas previamente por la función)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_imp = imputing_data(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#review imputed data #Values for Revision 1 for vitamin D\n",
    "X_train_imp[1]['Vitamin-D']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtenemos las **dos primeras revisiones** ya que nos es más sencillo trabajar. El resultado obtenido será extrapolable a un mayor número de revisiones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the two first reviews\n",
    "\n",
    "X_train_1 = X_train_imp[0]\n",
    "X_train_2 = X_train_imp[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Oversampling:\n",
    "\n",
    "Este método utiliza la generación de datos sintéticos para aumentar el número de muestras en el conjunto de datos.\n",
    "Objetivo: aumentar la clase minoritaria para que el conjunto de datos se equilibre mediante la creación de observaciones sintéticas basadas en las observaciones minoritarias existentes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Debido al desbalanceo entre clases, aplicaremos técnicas de balanceo. En este primer caso vamos a aplicar Oversampling mediante SMOTE Y ADASYN\n",
    "\n",
    "El algoritmo **SMOTE** está parametrizado con k_neighbors (el número de vecinos más cercanos que considerará) y el número de nuevos puntos que desea crear. Cada paso del algoritmo:\n",
    "\n",
    "- Seleccione al azar un punto minoritario.\n",
    "- Seleccione aleatoriamente cualquiera de sus k_neighbors vecinos más cercanos que pertenezcan a la misma clase.\n",
    "- Especifique aleatoriamente un valor lambda en el rango [0, 1].\n",
    "- Genere y coloque un nuevo punto en el vector entre los dos puntos, ubicado lambda por ciento del camino desde el punto original."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=https://miro.medium.com/max/700/1*6UFpLFl59O9e3e38ffTXJQ.png>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE, ADASYN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_smote, y_t_smote = SMOTE(k_neighbors = 11).fit_resample(X_train_1, y_train)\n",
    "print(sorted(Counter(y_t_smote).items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(y_t_smote)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ADASYN** es una versión mejorada de Smote. Lo que hace es igual que SMOTE pero con una pequeña mejora. Después de crear esa muestra, agrega pequeños valores aleatorios a los puntos, lo que la hace más realista. En otras palabras, en lugar de que toda la muestra se correlacione linealmente con el punto vecino, tienen un poco más de varianza, es decir, están un poco dispersas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_adasyn,y_train_adasyn = ADASYN(n_neighbors = 11).fit_resample(X_train_1,y_train)\n",
    "counter = sorted(Counter(y_train_adasyn).items())\n",
    "print(sorted(Counter(y_train_adasyn).items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(y_train_adasyn)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para ver los hiperparámetros disponibles, podemos crear un bosque aleatorio y examinar los valores predeterminados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at parameters used by our current forest\n",
    "print('Parameters currently in use:\\n')\n",
    "print(rf.get_params())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Debemos probar una amplia gama de valores y ver qué funciona. Intentaremos ajustar el siguiente conjunto de hiperparámetros:\n",
    "\n",
    "- max_features = max number of features considered for splitting a node\n",
    "- min_samples_split = min number of data points placed in a node before the node is split\n",
    "- min_samples_leaf = min number of data points allowed in a leaf node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para usar **RandomizedSearchCV**, primero necesitamos crear una cuadrícula de parámetros para muestrear durante el ajuste:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt','log2']\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4,8,10,12,14,16,18,20]\n",
    "# Create the random grid\n",
    "random_grid = {'max_features': max_features,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,}\n",
    "print(random_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En cada iteración, el algoritmo elegirá una combinación diferente de las características. El beneficio de una búsqueda aleatoria es que no probamos todas las combinaciones, sino que seleccionamos al azar para muestrear una amplia gama de valores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Grid Search**\n",
    "- cv, que es el número de pliegues que se deben usar para la validación cruzada. \n",
    "\n",
    "Más pliegues de cv reducen las posibilidades de sobreajuste, pero aumentarlo aumentará el tiempo de ejecución."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First create the base model to tune\n",
    "rf_1 = RandomForestClassifier(n_estimators=100, criterion='gini',  \n",
    "                   bootstrap=True, oob_score=False, class_weight = 'balanced_subsample')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "# Use the random grid to search for best hyperparameters\n",
    "# Grid search of parameters, using 7 fold cross validation,\n",
    "\n",
    "grid_random = GridSearchCV(rf_1, random_grid, cv=7 , n_jobs = -1, verbose = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the random search model\n",
    "grid_random.fit(X_train_smote, y_t_smote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_random.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos un **nuevo modelo** teniendo en cuenta los hiperparámetros que nos devuelve Grid_Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_2 = RandomForestClassifier(n_estimators=5000, criterion='gini', max_features = grid_random.best_params_['max_features'],\n",
    "                min_samples_leaf = grid_random.best_params_['min_samples_leaf'], min_samples_split = grid_random.best_params_['min_samples_split'],    \n",
    "                   bootstrap=True, oob_score=False, class_weight = 'balanced_subsample')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_2.fit(X_train_smote, y_t_smote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = rf_2.predict(X_train_smote)\n",
    "\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prestaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, plot_confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cm = confusion_matrix(y_train_adasyn,y_pred)\n",
    "cm = confusion_matrix(y_t_smote,y_pred)\n",
    "\n",
    "#plot_confusion_matrix(rf_1,X_train_adasyn,y_train_adasyn,cmap=plt.cm.Blues)\n",
    "plot_confusion_matrix(rf_2,X_train_smote,y_t_smote,cmap=plt.cm.Blues)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "#sensibilidad y especificidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#modify NaNs in X_test\n",
    "\n",
    "feature_names = ['Age','Weight','Size','IMC','Creatinine','Cystatin','HDL','LDL','Triglyciredes','GOT','GPT','GGT','Albuminuria','Ferritin','HOMA','Insulin','Blood_Glucose','Glycated-HB','PCR','Vitamin-D','TAS','TAD','Date']\n",
    "\n",
    "X_test_1 = pd.DataFrame(X_test[:,0,:],columns = feature_names[:-1])\n",
    "\n",
    "X_test_1 = X_test_1.drop('Blood_Glucose',axis = 1)\n",
    "X_test_1 = X_test_1.drop('Glycated-HB',axis = 1)\n",
    "\n",
    "#impute\n",
    "\n",
    "X_test_1 = X_test_1.fillna(X_train_1.median())\n",
    "\n",
    "y_pred_test = rf_2.predict(X_test_1)\n",
    "\n",
    "plot_confusion_matrix(rf_2,X_test_1,y_test,cmap=plt.cm.Blues)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aplicamos el modelo eliminando los variables Vitamin-D, HOMA e Insulin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_nan_review_analysis_1(X):\n",
    "    \"\"\"\n",
    "    Function that takes as input a matrix with pats and features for one review, and gets back\n",
    "    X_train_rev_imputed,pats_to_drop,feature_to_drop,imputation_data\n",
    "    \"\"\"\n",
    "    feature_names = ['Age','Weight','Size','IMC','Creatinine','Cystatin','HDL','LDL','Triglyciredes','GOT','GPT','GGT','Albuminuria','Ferritin','HOMA','Insulin','Blood_Glucose','Glycated-HB','PCR','Vitamin-D','TAS','TAD','Date']\n",
    "    \n",
    "    df = pd.DataFrame(X,columns = feature_names[:-1]) #convert it to df #delete date [:-1]\n",
    "    \n",
    "    print(df.columns)\n",
    "    df = df.drop('HOMA',axis = 1)\n",
    "    df = df.drop('Insulin',axis = 1)\n",
    "    df = df.drop('Blood_Glucose',axis = 1)\n",
    "    df = df.drop('Glycated-HB',axis = 1)\n",
    "    df = df.drop('Vitamin-D',axis = 1)\n",
    "    \n",
    "    \n",
    "    \n",
    "    #initialize vars to return\n",
    "    X_train_rev_imputed = []\n",
    "    pats_to_drop = []\n",
    "    feature_to_drop = []\n",
    "    imputation_data = []\n",
    "    #check number of NaNs per feature\n",
    "    \n",
    "    for index,value in enumerate (df.isnull().sum()):\n",
    "        if value !=0:\n",
    "            print(df.columns[index],value)\n",
    "            \n",
    "    #Drop features: Blood_Glucose and Glycated_HB\n",
    "    \n",
    "\n",
    "    ##reemplazar cada valor por la mediana \n",
    "    \n",
    "    X_train_rev_imputed = df.fillna(df.median()) \n",
    "    \n",
    "    return X_train_rev_imputed,pats_to_drop,feature_to_drop,imputation_data\n",
    "\n",
    "def imputing_data_1(X):\n",
    "    \"\"\" \n",
    "    Function that check data with nans and replace them with appropriate imputation\n",
    "    \"\"\"\n",
    "    X_train_aux = X_train.copy()\n",
    "    num_review = X_train_aux.shape[1]\n",
    "    \n",
    "    X_train_imp = []\n",
    "    \n",
    "    #run over the reviews and get the X_train_imputed, pats_to_drop, feature_to_drop, and imputation values\n",
    "    for i in range(3):\n",
    "        print(\"review\",i)\n",
    "        print(\"----------\")\n",
    "        X_train_rev_imputed,pats_to_drop,feature_to_drop,imputation_data = data_nan_review_analysis_1(X_train_aux[:,i,:])\n",
    "        print(\"----------\")\n",
    "        print(\"----------\")\n",
    "        \n",
    "        X_train_imp.append(X_train_rev_imputed)\n",
    "        \n",
    "    return X_train_imp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_imp_1 = imputing_data_1(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the two first reviews\n",
    "\n",
    "X_train_1 = X_train_imp_1[0]\n",
    "X_train_2 = X_train_imp_1[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE, ADASYN\n",
    "\n",
    "X_train_adasyn_1,y_train_adasyn_1 = ADASYN(n_neighbors = 11).fit_resample(X_train_1,y_train)\n",
    "print(sorted(Counter(y_train_adasyn_1).items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_1 = RandomForestClassifier(n_estimators=100, criterion='gini', min_samples_split=2, min_samples_leaf=1, \n",
    "                    max_features='auto', bootstrap=True, oob_score=True,class_weight = 'balanced_subsample')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_1.fit(X_train_adasyn_1, y_train_adasyn_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = rf_1.predict(X_train_adasyn_1)\n",
    "\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_train_adasyn_1,y_pred)\n",
    "\n",
    "plot_confusion_matrix(rf_1,X_train_adasyn_1,y_train_adasyn_1,cmap=plt.cm.Blues)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#modify NaNs in X_test\n",
    "\n",
    "feature_names = ['Age','Weight','Size','IMC','Creatinine','Cystatin','HDL','LDL','Triglyciredes','GOT','GPT','GGT','Albuminuria','Ferritin','HOMA','Insulin','Blood_Glucose','Glycated-HB','PCR','Vitamin-D','TAS','TAD','Date']\n",
    "\n",
    "X_test_1 = pd.DataFrame(X_test[:,0,:],columns = feature_names[:-1])\n",
    "\n",
    "\n",
    "X_test_1 = X_test_1.drop('Blood_Glucose',axis = 1)\n",
    "X_test_1 = X_test_1.drop('Glycated-HB',axis = 1)\n",
    "X_test_1 = X_test_1.drop('HOMA',axis = 1)\n",
    "X_test_1 = X_test_1.drop('Insulin',axis = 1)\n",
    "X_test_1 = X_test_1.drop('Vitamin-D',axis = 1)\n",
    "\n",
    "\n",
    "\n",
    "#impute\n",
    "\n",
    "X_test_1 = X_test_1.fillna(X_train_1.median())\n",
    "\n",
    "y_pred_test = rf_1.predict(X_test_1)\n",
    "\n",
    "plot_confusion_matrix(rf_1,X_test_1,y_test,cmap=plt.cm.Blues)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cosas que faltarían por hacer\n",
    "\n",
    "1. Habría que hacer una búsqueda de hiperparámetros utilizando oob score, sobre los parámetros que hablamos el otro día.\n",
    "1. Habría que comparar los resultados con la otra técnica de hacer oversampling. Quizá convendría utilizar alguna técnica diferente como undersampling, me parece que los resultados van a estar superajustados en otro caso.\n",
    "1. Hacer un calibrado de la probabilidad de predicción.\n",
    "1. Yo haría una prueba eliminando las variables: Vitamin-D, HOMA, Insulin y ver que sale sin ellas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
