{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ywen2\\Anaconda3\\lib\\site-packages\\statsmodels\\tools\\_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
      "  import pandas.util.testing as tm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pylab\n",
    "import scipy.stats as ss\n",
    "import csv\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import glob\n",
    "from io import StringIO\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import datasets\n",
    "from sklearn import svm\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. PREPROCESADO:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Cargamos las matrices anteriormente creadas** y que fueron divididas en train y test. Cargamos tanto los pacientes con sus variables y revisiones como las etiquetas asociadas a cada paciente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.load('Conjunto_train.npy')\n",
    "X_test = np.load('Conjunto_test.npy')\n",
    "y_train = np.load('Labels_train.npy')\n",
    "y_test = np.load('Labels_test.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Comprobamos las dimensiones de cada matriz**, tenemos 1317 pacientes para train y 330 para test (partición 80/20).\n",
    "\n",
    "El 13 hace referencia al número de revisiones que nosotros mismos decicimos como idóneas. 22 son el número de features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1317,)\n",
      "(1317, 13, 22)\n",
      "(330,)\n",
      "(330, 13, 22)\n"
     ]
    }
   ],
   "source": [
    "print(y_train.shape)\n",
    "print(X_train.shape)\n",
    "print(y_test.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comenzaremos **con dos instantes temporales** (nos será más fácil trabajar así, será extrapolable a cuando tengamos un mayor número de revisiones):\n",
    "\n",
    "- X_train_1[:,0,:] hace referencia al primer instante temporal (1º revisión), cogemos todos los pacientes y todas las features para ese primer instante.\n",
    "\n",
    "- X_train_1[:,1,:] hace referencia al segundo instante temporal (2º revisión), cogemos todos los pacientes y todas las features para ese segundo instante."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De momento cargaremos solo la primera revisión, ya que trabajaremos con la segunda en otro notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_1 = X_train[:,0,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Como observamos el el histograma, los **datos están desbalanceados**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(y_train)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a hacer lo siguiente en la función:\n",
    "\n",
    "1. Revisar si hay alguna característica que tenga más del 50% a NaN, entonces habría que considerar eliminarla. Tenemos un problema, porque hay que revisar que tengo más de 50% en todas las revisiones. Para no complicar las cosas, vamos a considerar que trabajamos sólo con tres revisiones (vamos a ver si llegamos rápido a ese punto, en otro caso deberíamos ver si paramos en tres revisiones, que ya es bastante).\n",
    "\n",
    "1. Revisar si hay algún paciente con más de 8 características NaN, con que esto ocurra en cualquiera de las tres revisiones, deberíamos considerarlo que se elimina, en todas las revisiones.\n",
    "\n",
    "## 1.1 Features con problemas:\n",
    "\n",
    "Revisando la información rápidamente hay varias features con problemas:\n",
    "\n",
    "1. **Importante**: las variables *Blood_Glucose* y *Glycated-HB* hay que sacarlas fuera de la base de datos, porque son las que se utilizan para determinar si un pacientes diabético o no.\n",
    "1. Hay cuatro variables que tiene muchos NaN, así que lo que intentaría sería centrarnos en cómo imputarlas, tanto en training como en test. Esto es un poco más complicado de lo que hacemos habitualmente con las bases de datos normales, porque aquí tenemos más datos y de forma un poco más complicada.\n",
    "    * HOMA\n",
    "    * Insulin\n",
    "    * Vitamin-D\n",
    "   \n",
    "Estas son las que más tienen en las tres primeras revisiones. Lo que vamos a hacer es lo siguiente, aprovechando que tenemos un montón de datos y datos temporales vamos a utilizar un esquema diferente para hacer la imputación de forma que sea más razonable, intentando perder el menor número de datos posibles, por ejemplo: \n",
    " * Regresión lineal múltiple con respecto a todas las variables temporales. Otra cosa\n",
    " * Verificar cuál es la correlación entre variables, para una misma revisión, y buscar con las que mayor correlación tengan y proponer un modelo de regresión lineal. \n",
    " \n",
    "Mi propuesta para el TFG: \n",
    " * Vamos a hacer imputación por la mediana (en el caso de Vitamin-D, HOMA e Insulin) casi seguro que vamos a empeorar un poco porque hay muchos NaN, pero la esperanza es que todo se compense.\n",
    " * Si te ves con fuerzas y ganas probamos lo que he comentado anteriormente\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = ['Age','Weight','Size','IMC','Creatinine','Cystatin','HDL','LDL','Triglyciredes','GOT','GPT','GGT','Albuminuria','Ferritin','HOMA','Insulin','Blood_Glucose','Glycated-HB','PCR','Vitamin-D','TAS','TAD','Date']\n",
    "    \n",
    "df = pd.DataFrame(X_train_1,columns = feature_names[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight 15\n",
      "Size 6\n",
      "IMC 19\n",
      "Creatinine 21\n",
      "Cystatin 263\n",
      "HDL 115\n",
      "LDL 135\n",
      "Triglyciredes 24\n",
      "GOT 198\n",
      "GPT 26\n",
      "GGT 54\n",
      "Albuminuria 209\n",
      "Ferritin 133\n",
      "HOMA 958\n",
      "Insulin 939\n",
      "Blood_Glucose 27\n",
      "Glycated-HB 740\n",
      "PCR 231\n",
      "Vitamin-D 1079\n"
     ]
    }
   ],
   "source": [
    "for index,value in enumerate (df.isnull().sum()):\n",
    "        if value !=0:\n",
    "            print(df.columns[index],value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_nan_review_analysis(X):\n",
    "    \"\"\"\n",
    "    Function that takes as input a matrix with pats and features for one review, and gets back\n",
    "    X_train_rev_imputed,pats_to_drop,feature_to_drop,imputation_data\n",
    "    \"\"\"\n",
    "    feature_names = ['Age','Weight','Size','IMC','Creatinine','Cystatin','HDL','LDL','Triglyciredes','GOT','GPT','GGT','Albuminuria','Ferritin','HOMA','Insulin','Blood_Glucose','Glycated-HB','PCR','Vitamin-D','TAS','TAD','Date']\n",
    "    \n",
    "    df = pd.DataFrame(X,columns = feature_names[:-1]) #convert it to df #delete date [:-1]\n",
    "    \n",
    "    print(df.columns)\n",
    "    df = df.drop('Blood_Glucose',axis = 1)\n",
    "    df = df.drop('Glycated-HB',axis = 1)\n",
    "    \n",
    "    \n",
    "    #initialize vars to return\n",
    "    X_train_rev_imputed = []\n",
    "    pats_to_drop = []\n",
    "    feature_to_drop = []\n",
    "    imputation_data = []\n",
    "    #check number of NaNs per feature\n",
    "    \n",
    "    for index,value in enumerate (df.isnull().sum()):\n",
    "        if value !=0:\n",
    "            print(df.columns[index],value)\n",
    "            \n",
    "    #Drop features: Blood_Glucose and Glycated_HB\n",
    "    \n",
    "\n",
    "    ##reemplazar cada valor por la mediana \n",
    "    \n",
    "    X_train_rev_imputed = df.fillna(df.median()) \n",
    "    \n",
    "    return X_train_rev_imputed,pats_to_drop,feature_to_drop,imputation_data\n",
    "\n",
    "def imputing_data(X):\n",
    "    \"\"\" \n",
    "    Function that check data with nans and replace them with appropriate imputation\n",
    "    \"\"\"\n",
    "    X_train_aux = X_train.copy()\n",
    "    num_review = X_train_aux.shape[1]\n",
    "    \n",
    "    X_train_imp = []\n",
    "    \n",
    "    #run over the reviews and get the X_train_imputed, pats_to_drop, feature_to_drop, and imputation values\n",
    "    for i in range(3):\n",
    "        print(\"review\",i)\n",
    "        print(\"----------\")\n",
    "        X_train_rev_imputed,pats_to_drop,feature_to_drop,imputation_data = data_nan_review_analysis(X_train_aux[:,i,:])\n",
    "        print(\"----------\")\n",
    "        print(\"----------\")\n",
    "        \n",
    "        X_train_imp.append(X_train_rev_imputed)\n",
    "        \n",
    "    return X_train_imp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos una nueva variable: **X_train_imp**, obtenida de aplicar la función imputing_data a X_train. Devuelve lo siguiente\n",
    "\n",
    "- Nº de revisión\n",
    "- Features\n",
    "- Features con sus respectivos NaN (no aparecen las features eliminadas previamente por la función)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "review 0\n",
      "----------\n",
      "Index(['Age', 'Weight', 'Size', 'IMC', 'Creatinine', 'Cystatin', 'HDL', 'LDL',\n",
      "       'Triglyciredes', 'GOT', 'GPT', 'GGT', 'Albuminuria', 'Ferritin', 'HOMA',\n",
      "       'Insulin', 'Blood_Glucose', 'Glycated-HB', 'PCR', 'Vitamin-D', 'TAS',\n",
      "       'TAD'],\n",
      "      dtype='object')\n",
      "Weight 15\n",
      "Size 6\n",
      "IMC 19\n",
      "Creatinine 21\n",
      "Cystatin 263\n",
      "HDL 115\n",
      "LDL 135\n",
      "Triglyciredes 24\n",
      "GOT 198\n",
      "GPT 26\n",
      "GGT 54\n",
      "Albuminuria 209\n",
      "Ferritin 133\n",
      "HOMA 958\n",
      "Insulin 939\n",
      "PCR 231\n",
      "Vitamin-D 1079\n",
      "----------\n",
      "----------\n",
      "review 1\n",
      "----------\n",
      "Index(['Age', 'Weight', 'Size', 'IMC', 'Creatinine', 'Cystatin', 'HDL', 'LDL',\n",
      "       'Triglyciredes', 'GOT', 'GPT', 'GGT', 'Albuminuria', 'Ferritin', 'HOMA',\n",
      "       'Insulin', 'Blood_Glucose', 'Glycated-HB', 'PCR', 'Vitamin-D', 'TAS',\n",
      "       'TAD'],\n",
      "      dtype='object')\n",
      "Age 2\n",
      "Weight 13\n",
      "Size 4\n",
      "IMC 15\n",
      "Creatinine 5\n",
      "Cystatin 141\n",
      "HDL 100\n",
      "LDL 119\n",
      "Triglyciredes 8\n",
      "GOT 200\n",
      "GPT 8\n",
      "GGT 35\n",
      "Albuminuria 166\n",
      "Ferritin 81\n",
      "HOMA 919\n",
      "Insulin 903\n",
      "PCR 132\n",
      "Vitamin-D 993\n",
      "----------\n",
      "----------\n",
      "review 2\n",
      "----------\n",
      "Index(['Age', 'Weight', 'Size', 'IMC', 'Creatinine', 'Cystatin', 'HDL', 'LDL',\n",
      "       'Triglyciredes', 'GOT', 'GPT', 'GGT', 'Albuminuria', 'Ferritin', 'HOMA',\n",
      "       'Insulin', 'Blood_Glucose', 'Glycated-HB', 'PCR', 'Vitamin-D', 'TAS',\n",
      "       'TAD'],\n",
      "      dtype='object')\n",
      "Age 112\n",
      "Weight 121\n",
      "Size 112\n",
      "IMC 123\n",
      "Creatinine 119\n",
      "Cystatin 250\n",
      "HDL 206\n",
      "LDL 227\n",
      "Triglyciredes 126\n",
      "GOT 338\n",
      "GPT 124\n",
      "GGT 147\n",
      "Albuminuria 276\n",
      "Ferritin 186\n",
      "HOMA 973\n",
      "Insulin 960\n",
      "PCR 252\n",
      "Vitamin-D 1001\n",
      "TAS 109\n",
      "TAD 109\n",
      "----------\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "X_train_imp = imputing_data(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#review imputed data #Values for Revision 1 for vitamin D\n",
    "#X_train_imp[1]['Vitamin-D']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Obtenemos las **dos primeras revisiones** ya que nos es más sencillo trabajar. El resultado obtenido será extrapolable a un mayor número de revisiones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the two first reviews\n",
    "\n",
    "X_train_1_imp = X_train_imp[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Undersampling:\n",
    "\n",
    "Las técnicas de *undersampling* eliminan ejemplos del conjunto de datos de entrenamiento que pertenecen a la clase mayoritaria para equilibrar mejor la distribución de clases.\n",
    "\n",
    "La técnica de submuestreo más simple implica la selección aleatoria de ejemplos de la clase mayoritaria y su eliminación del conjunto de datos de entrenamiento. Esto se conoce como submuestreo aleatorio. Aunque simple y eficaz, una limitación de esta técnica es que los ejemplos se eliminan sin preocuparse por su utilidad o importancia para determinar el límite de decisión entre las clases. Esto significa que es posible, o incluso probable, que se elimine información útil.\n",
    "\n",
    "Una extensión de este enfoque es ser más exigente con respecto a los ejemplos de la clase mayoritaria que se eliminan. Por lo general, se trata de modelos heurísticos o de aprendizaje que intentan identificar ejemplos redundantes para la eliminación o ejemplos útiles para la no eliminación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. PREDICCIÓN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Debemos probar una amplia gama de valores y ver qué funciona. Intentaremos ajustar el siguiente conjunto de hiperparámetros:\n",
    "\n",
    "- max_features = max number of features considered for splitting a node\n",
    "- min_samples_split = min number of data points placed in a node before the node is split\n",
    "- min_samples_leaf = min number of data points allowed in a leaf node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para usar **RandomizedSearchCV**, primero necesitamos crear una cuadrícula de parámetros para muestrear durante el ajuste:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_features': ['auto', 'sqrt', 'log2'], 'min_samples_split': [2, 5, 10], 'min_samples_leaf': [1, 2, 4, 8, 10, 12, 14, 16, 18, 20]}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt','log2']\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4,8,10,12,14,16,18,20]\n",
    "# Create the random grid\n",
    "random_grid = {'max_features': max_features,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,}\n",
    "print(random_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En cada iteración, el algoritmo elegirá una combinación diferente de las características. El beneficio de una búsqueda aleatoria es que no probamos todas las combinaciones, sino que seleccionamos al azar para muestrear una amplia gama de valores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Grid Search**\n",
    "\n",
    "GridSearchCV es una clase disponible en scikit-learn que permite evaluar y seleccionar de forma sistemática los parámetros de un modelo. Indicándole un modelo y los parámetros a probar, puede evaluar el rendimiento del primero en función de los segundos mediante validación cruzada.\n",
    "\n",
    "- cv, que es el número de pliegues que se deben usar para la validación cruzada. \n",
    "\n",
    "Más pliegues de cv reducen las posibilidades de sobreajuste, pero aumentarlo aumentará el tiempo de ejecución."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Balanced Random Forest** es una modificación de RF, donde por cada árbol se construyen dos conjuntos bootstrap del mismo tamaño, igual al tamaño de la clase minoritaria: uno para la clase minoritaria, el otro para la clase mayoritaria. Conjuntamente, estos dos conjuntos constituyen el conjunto de entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First create the base model to tune\n",
    "\n",
    "#Creo que es mejor la estrategia que se presenta en from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "\n",
    "rf_1 = BalancedRandomForestClassifier(n_estimators=100, criterion='gini',  \n",
    "                   bootstrap=True, oob_score=False, class_weight = 'balanced_subsample')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "# Use the random grid to search for best hyperparameters\n",
    "# Grid search of parameters, using 7 fold cross validation,\n",
    "\n",
    "grid_random = GridSearchCV(rf_1, random_grid, cv=7 , n_jobs = -1, verbose = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 7 folds for each of 90 candidates, totalling 630 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=7,\n",
       "             estimator=BalancedRandomForestClassifier(class_weight='balanced_subsample'),\n",
       "             n_jobs=-1,\n",
       "             param_grid={'max_features': ['auto', 'sqrt', 'log2'],\n",
       "                         'min_samples_leaf': [1, 2, 4, 8, 10, 12, 14, 16, 18,\n",
       "                                              20],\n",
       "                         'min_samples_split': [2, 5, 10]},\n",
       "             verbose=5)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the random search model\n",
    "grid_random.fit(X_train_1_imp, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_features': 'sqrt', 'min_samples_leaf': 2, 'min_samples_split': 2}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_random.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos un **nuevo modelo** teniendo en cuenta los hiperparámetros que nos devuelve Grid_Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_2 = BalancedRandomForestClassifier(n_estimators=5000, criterion='gini', max_features = grid_random.best_params_['max_features'],\n",
    "                min_samples_leaf = grid_random.best_params_['min_samples_leaf'], min_samples_split = grid_random.best_params_['min_samples_split'],    \n",
    "                   bootstrap=True, oob_score=False, class_weight = 'balanced_subsample')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BalancedRandomForestClassifier(class_weight='balanced_subsample',\n",
       "                               max_features='sqrt', min_samples_leaf=2,\n",
       "                               n_estimators=5000)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_2.fit(X_train_1_imp, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = rf_2.predict(X_train_1_imp)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_prob = rf_2.predict_proba(X_train_1_imp)\n",
    "print(y_pred_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Prestaciones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La **sensibilidad y la especificidad** son dos valores que nos indican la capacidad de nuestro estimador para discriminar los casos positivos, de los negativos. La sensibilidad es la fracción de verdaderos positivos, mientras que la especificidad, es la fracción de verdaderos negativos.\n",
    "\n",
    "-  **La sensibilidad** es la proporción de casos positivos que fueron correctamente identificadas por el algoritmo.\n",
    "-  **La especificidad** se trata de los casos negativos que el algoritmo ha clasificado correctamente.  Expresa cuan bien puede el modelo detectar esa clase.\n",
    "- **La Precisión** se refiere a la dispersión del conjunto de valores obtenidos a partir de mediciones repetidas de una magnitud. Cuanto menor es la dispersión mayor la precisión. Se representa por la proporción entre el número de predicciones correctas (tanto positivas como negativas) y el total de predicciones. En forma práctica es  el porcentaje de casos positivos detectados.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prestaciones en train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, plot_confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_train,y_pred)\n",
    "\n",
    "plot_confusion_matrix(rf_2,X_train_1_imp,y_train,cmap=plt.cm.Blues)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "total1=sum(sum(cm))\n",
    "\n",
    "accuracy1=(cm[0,0]+cm[1,1])/total1\n",
    "print ('Accuracy : ', accuracy1)\n",
    "\n",
    "sensitivity1 = cm[1,1]/(cm[1,0]+cm[1,1])\n",
    "print('Sensitivity : ', sensitivity1 )\n",
    "\n",
    "specificity1 = cm[0,0]/(cm[0,0]+cm[0,1])  #ojo, creo que está al revés esto\n",
    "print('Specificity : ', specificity1)\n",
    "\n",
    "from imblearn.metrics import classification_report_imbalanced\n",
    "\n",
    "print(classification_report_imbalanced(y_train, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prestaciones en test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#modify NaNs in X_test\n",
    "\n",
    "feature_names = ['Age','Weight','Size','IMC','Creatinine','Cystatin','HDL','LDL','Triglyciredes','GOT','GPT','GGT','Albuminuria','Ferritin','HOMA','Insulin','Blood_Glucose','Glycated-HB','PCR','Vitamin-D','TAS','TAD','Date']\n",
    "\n",
    "X_test_1 = pd.DataFrame(X_test[:,0,:],columns = feature_names[:-1])\n",
    "\n",
    "X_test_1 = X_test_1.drop('Blood_Glucose',axis = 1)\n",
    "X_test_1 = X_test_1.drop('Glycated-HB',axis = 1)\n",
    "\n",
    "#impute\n",
    "\n",
    "X_test_1 = X_test_1.fillna(X_train_1_imp.median())\n",
    "\n",
    "y_pred_test = rf_2.predict(X_test_1)\n",
    "y_pred_test_prob = rf_2.predict_proba(X_test_1)\n",
    "\n",
    "plot_confusion_matrix(rf_2,X_test_1,y_test,cmap=plt.cm.Blues)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm1 = confusion_matrix(y_test,y_pred_test)\n",
    "\n",
    "total1=sum(sum(cm1))\n",
    "\n",
    "accuracy1=(cm1[0,0]+cm1[1,1])/total1\n",
    "print ('Accuracy : ', accuracy1)\n",
    "\n",
    "specificity1 = cm1[0,0]/(cm1[0,0]+cm1[0,1])\n",
    "print('Specificity : ', specificity1 )\n",
    "\n",
    "sensitivity1 = cm1[1,1]/(cm1[1,0]+cm1[1,1])\n",
    "print('Sensitivity : ', sensitivity1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Accuracy :  0.5606060606060606\n",
    "Specificity :  0.5478547854785478\n",
    "Sensitivity :  0.7037037037037037"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aplicamos el modelo eliminando las variables Vitamin-D, HOMA e Insulin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. PREPROCESADO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_nan_review_analysis_1(X):\n",
    "    \"\"\"\n",
    "    Function that takes as input a matrix with pats and features for one review, and gets back\n",
    "    X_train_rev_imputed,pats_to_drop,feature_to_drop,imputation_data\n",
    "    \"\"\"\n",
    "    feature_names = ['Age','Weight','Size','IMC','Creatinine','Cystatin','HDL','LDL','Triglyciredes','GOT','GPT','GGT','Albuminuria','Ferritin','HOMA','Insulin','Blood_Glucose','Glycated-HB','PCR','Vitamin-D','TAS','TAD','Date']\n",
    "    \n",
    "    df = pd.DataFrame(X,columns = feature_names[:-1]) #convert it to df #delete date [:-1]\n",
    "    \n",
    "    print(df.columns)\n",
    "    df = df.drop('HOMA',axis = 1)\n",
    "    df = df.drop('Insulin',axis = 1)\n",
    "    df = df.drop('Blood_Glucose',axis = 1)\n",
    "    df = df.drop('Glycated-HB',axis = 1)\n",
    "    df = df.drop('Vitamin-D',axis = 1)\n",
    "    \n",
    "    \n",
    "    \n",
    "    #initialize vars to return\n",
    "    X_train_rev_imputed = []\n",
    "    pats_to_drop = []\n",
    "    feature_to_drop = []\n",
    "    imputation_data = []\n",
    "    #check number of NaNs per feature\n",
    "    \n",
    "    for index,value in enumerate (df.isnull().sum()):\n",
    "        if value !=0:\n",
    "            print(df.columns[index],value)\n",
    "            \n",
    "    #Drop features: Blood_Glucose and Glycated_HB\n",
    "    \n",
    "\n",
    "    ##reemplazar cada valor por la mediana \n",
    "    \n",
    "    X_train_rev_imputed = df.fillna(df.median()) \n",
    "    \n",
    "    return X_train_rev_imputed,pats_to_drop,feature_to_drop,imputation_data\n",
    "\n",
    "def imputing_data_1(X):\n",
    "    \"\"\" \n",
    "    Function that check data with nans and replace them with appropriate imputation\n",
    "    \"\"\"\n",
    "    X_train_aux = X_train.copy()\n",
    "    num_review = X_train_aux.shape[1]\n",
    "    \n",
    "    X_train_imp = []\n",
    "    \n",
    "    #run over the reviews and get the X_train_imputed, pats_to_drop, feature_to_drop, and imputation values\n",
    "    for i in range(3):\n",
    "        print(\"review\",i)\n",
    "        print(\"----------\")\n",
    "        X_train_rev_imputed,pats_to_drop,feature_to_drop,imputation_data = data_nan_review_analysis_1(X_train_aux[:,i,:])\n",
    "        print(\"----------\")\n",
    "        print(\"----------\")\n",
    "        \n",
    "        X_train_imp.append(X_train_rev_imputed)\n",
    "        \n",
    "    return X_train_imp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_imp_1 = imputing_data_1(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the first reviews\n",
    "\n",
    "X_train_11_imp = X_train_imp_1[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. PREDICCIÓN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt','log2']\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4,8,10,12,14,16,18,20]\n",
    "# Create the random grid\n",
    "random_grid = {'max_features': max_features,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,}\n",
    "print(random_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First create the base model to tune\n",
    "rf_11 = BalancedRandomForestClassifier(n_estimators=100, criterion='gini',  \n",
    "                   bootstrap=True, oob_score=False, class_weight = 'balanced_subsample')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "# Use the random grid to search for best hyperparameters\n",
    "# Grid search of parameters, using 7 fold cross validation,\n",
    "\n",
    "grid_random = GridSearchCV(rf_11, random_grid, cv=7 , n_jobs = -1, verbose = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the random search model\n",
    "grid_random.fit(X_train_11_imp, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_random.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rebalancea dentro de cada árbol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_22 = BalancedRandomForestClassifier(n_estimators=5000, criterion='gini', max_features = grid_random.best_params_['max_features'],\n",
    "                min_samples_leaf = grid_random.best_params_['min_samples_leaf'], min_samples_split = grid_random.best_params_['min_samples_split'],    \n",
    "                   bootstrap=True, oob_score=False, class_weight = 'balanced_subsample')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_22.fit(X_train_11_imp, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_1 = rf_22.predict(X_train_11_imp)\n",
    "y_pred_1_prob = rf_22.predict_proba(X_train_11_imp)\n",
    "print(y_pred_1)\n",
    "print(y_pred_1_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Calibración de la probabilidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "base_clf = BalancedRandomForestClassifier(n_estimators=50, criterion='gini', max_features = grid_random.best_params_['max_features'],\n",
    "                min_samples_leaf = grid_random.best_params_['min_samples_leaf'], min_samples_split = grid_random.best_params_['min_samples_split'],    \n",
    "                   bootstrap=True, oob_score=False, class_weight = 'balanced_subsample')\n",
    "\n",
    "#vamos a calibrar training\n",
    "calibrated_clf = CalibratedClassifierCV(base_estimator=rf_22,cv='prefit', method = 'sigmoid',ensemble = False)\n",
    "\n",
    "calibrated_clf.fit(X_train_11, y_train)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''#calibrated_clf.\n",
    "from sklearn.calibration import calibration_curve\n",
    "\n",
    "feature_names = ['Age','Weight','Size','IMC','Creatinine','Cystatin','HDL','LDL','Triglyciredes','GOT','GPT','GGT','Albuminuria','Ferritin','HOMA','Insulin','Blood_Glucose','Glycated-HB','PCR','Vitamin-D','TAS','TAD','Date']\n",
    "\n",
    "X_test_1 = pd.DataFrame(X_test[:,0,:],columns = feature_names[:-1])\n",
    "\n",
    "X_test_1 = X_test_1.drop('Blood_Glucose',axis = 1)\n",
    "X_test_1 = X_test_1.drop('Glycated-HB',axis = 1)\n",
    "X_test_1 = X_test_1.drop('HOMA',axis = 1)\n",
    "X_test_1 = X_test_1.drop('Insulin',axis = 1)\n",
    "X_test_1 = X_test_1.drop('Vitamin-D',axis = 1)\n",
    "\n",
    "X_test_1 = X_test_1.fillna(X_train_1.median())\n",
    "\n",
    "\n",
    "y_pred_cali = calibrated_clf.predict_proba(X_test_1)[:,1]\n",
    "\n",
    "y_train_cali = calibrated_clf.predict_proba(X_train_11)[:,1]\n",
    "\n",
    "fraction_of_positives, mean_predicted_value = calibration_curve(y_test, y_pred_cali, n_bins=10)\n",
    "fraction_of_positives_train, mean_predicted_value_train = calibration_curve(y_train, y_train_cali, n_bins=10)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot([0, 1], [0, 1], \"k:\", label=\"Perfectly calibrated\")\n",
    "plt.plot(mean_predicted_value, fraction_of_positives, \"s-\")\n",
    "plt.plot(mean_predicted_value_train, fraction_of_positives_train, \"^-\")\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(y_pred_cali, range=(0, 1), bins=10,histtype=\"step\", lw=2)\n",
    "plt.hist(y_train_cali, range=(0, 1), bins=10,histtype=\"step\", lw=2)\n",
    "\n",
    "\n",
    "plt.hist(rf_22.predict_proba(X_train_11)[:,1],bins = 10,histtype='step')'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Prestaciones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prestaciones en train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, plot_confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cm = confusion_matrix(y_train_adasyn,y_pred)\n",
    "\n",
    "cm = confusion_matrix(y_train,y_pred_1)\n",
    "print(cm)\n",
    "\n",
    "#plot_confusion_matrix(rf_1,X_train_adasyn,y_train_adasyn,cmap=plt.cm.Blues)\n",
    "plot_confusion_matrix(rf_22,X_train_11_imp,y_train,cmap=plt.cm.Blues)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "accuracy1=(cm[0,0]+cm[1,1])/(cm[0,0]+cm[1,1]+cm[0,1]+cm[1,0])\n",
    "print ('Accuracy : ', accuracy1)\n",
    "\n",
    "sensitivity1 = cm[1,1]/(cm[1,0]+cm[1,1])\n",
    "print('Sensitivity : ', sensitivity1 )\n",
    "\n",
    "specificity1 = cm[0,0]/(cm[0,0]+cm[0,1])  #ojo, creo que está al revés esto\n",
    "print('Specificity : ', specificity1)\n",
    "\n",
    "\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import plot_roc_curve\n",
    "\n",
    "plot_roc_curve(rf_22, X_train_11_imp, y_train)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prestaciones en test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#modify NaNs in X_test\n",
    "\n",
    "feature_names = ['Age','Weight','Size','IMC','Creatinine','Cystatin','HDL','LDL','Triglyciredes','GOT','GPT','GGT','Albuminuria','Ferritin','HOMA','Insulin','Blood_Glucose','Glycated-HB','PCR','Vitamin-D','TAS','TAD','Date']\n",
    "\n",
    "X_test_11 = pd.DataFrame(X_test[:,0,:],columns = feature_names[:-1])\n",
    "\n",
    "X_test_11 = X_test_11.drop('Blood_Glucose',axis = 1)\n",
    "X_test_11 = X_test_11.drop('Glycated-HB',axis = 1)\n",
    "X_test_11 = X_test_11.drop('HOMA',axis = 1)\n",
    "X_test_11 = X_test_11.drop('Insulin',axis = 1)\n",
    "X_test_11 = X_test_11.drop('Vitamin-D',axis = 1)\n",
    "\n",
    "#impute\n",
    "\n",
    "X_test_11 = X_test_11.fillna(X_train_imp_1.median())\n",
    "\n",
    "y_pred_test_1 = rf_22.predict(X_test_11)\n",
    "y_pred_test_prob_1 = rf_22.predict_proba(X_test_11) #Probabilidad de pertenecer a cada clase\n",
    "\n",
    "plot_confusion_matrix(rf_22,X_test_11,y_test,cmap=plt.cm.Blues)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(y_pred_test_prob[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm1 = confusion_matrix(y_test,y_pred_test)\n",
    "print(cm1)\n",
    "\n",
    "accuracy1=(cm[0,0]+cm[1,1])/(cm[0,0]+cm[1,1]+cm[0,1]+cm[1,0])\n",
    "print ('Accuracy : ', accuracy1)\n",
    "\n",
    "sensitivity1 = cm1[1,1]/(cm1[1,0]+cm1[1,1])\n",
    "print('Sensitivity : ', sensitivity1 )\n",
    "\n",
    "specificity1 = cm1[0,0]/(cm1[0,0]+cm1[0,1])  #ojo, creo que está al revés esto\n",
    "print('Specificity : ', specificity1)\n",
    "\n",
    "\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import plot_roc_curve\n",
    "\n",
    "plot_roc_curve(rf_22, X_test_11, y_test)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Guardamos todas las probabilidades para imputarlas como feature en la segunda revisión"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('Prob_train_review_1_all_features', y_pred)\n",
    "np.save('Prob_test_review_1_all_features', y_pred_test)\n",
    "np.save('Prob_train_review_1_without_homa_vit_d', y_pred_1)\n",
    "np.save('Prob_test_review_1_without_homa_vit_d', y_pred_test_1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
