{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pylab\n",
    "import scipy.stats as ss\n",
    "import csv\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import glob\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import datasets\n",
    "from sklearn import svm\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pd.read_csv('499.csv')\n",
    "#df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.to_csv(\"499.csv\", header=['EDAD', 'PESO','TALLA', 'IMC', 'CREATININA', 'CISTATINA', 'HDL', 'LDL', 'TRIGLICERIDOS', \n",
    "#'GOT', 'GPT', 'GGT', 'ALBUMINURIA', 'FERRITINA', 'HOMA', 'INSULINA', 'GLUCEMIA', \n",
    "#'HB-GLICOSILADA', 'PCR', 'VITAMINA-D', 'TAS', 'TAD', 'FECHA'], index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "all_data = pd.DataFrame() #initializes DF which will hold aggregated csv files\n",
    "\n",
    "#list of all df\n",
    "dfs = []\n",
    "for f in glob.glob(\"*.csv\"): #for all csv files in pwd\n",
    "       #add parameters to read_csv\n",
    "    df = pd.read_csv(f, header=None) #create dataframe for reading current csv\n",
    "       #print df\n",
    "    dfs.append(df) #appends current csv to final DF\n",
    "    all_data = pd.concat(dfs, ignore_index=True)\n",
    "print (all_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CONVERTIR PACIENTES (los 1700 csv) A UNA LISTA Y LA LISTA A CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Primero especificamos un patrón del archivo y lo pasamos como parámetro en la función glob\n",
    "csv_files = glob.glob('*.csv')\n",
    "csv_files.pop()\n",
    "\n",
    "new_csv_files =  [item.replace(\".csv\", \"\") for item in csv_files]\n",
    "#new_csv_files.pop()\n",
    "# Mostrar el archivo csv_files, el cual es una lista de nombres\n",
    "print(new_csv_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(new_csv_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(new_csv_files, columns=[\"Pacientes\"])\n",
    "df.to_csv('new_csv_files.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CREO UNA LISTA A PARTIR DEL CSV PACIENTES PROGRESORES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('pacientes_progresores.csv', 'r') as archivo:\n",
    "    leer = csv.reader(archivo)\n",
    "    lista = list(leer)\n",
    "    #print(lista)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista_HC = []\n",
    "for i in lista:\n",
    "    lista_HC.append(i[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista_HC.pop(0)\n",
    "#print(lista_HC)\n",
    "print(len(lista_HC))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### COMPARO AMBAS LISTAS Y ELIMINO PACIENTES NO COINCIDENTES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparacion = []\n",
    " \n",
    "for item in new_csv_files:\n",
    "    if item in lista_HC:\n",
    "        comparacion.append(item)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(comparacion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(comparacion, columns=[\"Pacientes\"])\n",
    "df.to_csv('pacientes_procesados.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AHORA TENGO UN CSV EN EL CUAL CADA LINEA ES CADA UNO DE LOS CSV INDEPENDIENTES QUE TENIA ANTERIORMENTE PARA CADA SUJETO, ¿COMO PUEDO HACER QUE CADA LINEA VUELVA A SER UN CSV INDEPENDIENTE Y ASI TENER 1647 PACIENTES TRAS HABER ELIMINADO LOS 100 Y PICO QUE YA ERAN DIABÉTICOS?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from contextlib import ExitStack\n",
    "import math\n",
    "\n",
    "# Generamos una función para leer las líneas de \n",
    "# un fichero.\n",
    "def get_lines(fname):\n",
    "    with open(fname, 'rt') as file:\n",
    "        lines = 0\n",
    "        for l in file: lines += 1\n",
    "    return lines\n",
    "\n",
    "# Función para dividir el fichero csv en varios\n",
    "def split_csv(path_csv, splitby):\n",
    "\n",
    "    with open(path_csv, 'r') as csvfile, ExitStack() as stack:\n",
    "        # Líneas totales del fichero\n",
    "        lines = get_lines(path_csv)\n",
    "\n",
    "        # número de líneas por fichero (excepto el último)\n",
    "        chunk_num_lines = math.floor(lines/splitby)\n",
    "        chunk_num_lines = chunk_num_lines if chunk_num_lines > 0 else 1\n",
    "\n",
    "        # Máximo de ficheros que podemos generar\n",
    "        # por si ponemos un número \"loco\"\n",
    "        max_files = min(lines, splitby)\n",
    "\n",
    "        # Archivos que vamos a crear\n",
    "        filenames = ['chunk{}.csv'.format(i) for i in range(0, max_files)]\n",
    "\n",
    "        # Ficheros (Chunks)\n",
    "        files = [ stack.enter_context(open(fname, 'wt')) for fname in filenames ]\n",
    "\n",
    "        # Índice del fihcero actual\n",
    "        file_index = 0\n",
    "        # Máximo índice\n",
    "        max_index = len(filenames) - 1\n",
    "\n",
    "        # Recorremos el fichero\n",
    "        for idx, line in enumerate(csvfile):\n",
    "            # Obtenemos el chunk sobre el que escribir\n",
    "            # en base al índice actual\n",
    "            file = files[file_index]\n",
    "            # Escribimos línea\n",
    "            file.write(line)\n",
    "\n",
    "            # Actualizamos el índice cuando hayamos escrito\n",
    "            # el máximo de líneas para el fichero\n",
    "            # escepto si file_index == max_index\n",
    "            if (idx + 1) % chunk_num_lines == 0 and file_index < max_index:\n",
    "                file_index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get_lines('pacientes_procesados.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split_csv('pacientes_procesados.csv', 1648)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_data = []\n",
    "  \n",
    "# Escribimos un loop que irá a través de cada uno de los nombres de archivo a través de globbing y el resultado final será la lista dataframes\n",
    "for filename in new_csv_files:\n",
    "    data = pd.read_csv(filename)\n",
    "    list_data.append(data)\n",
    "#Para chequear que todo está bien, mostramos la list_data por consola\n",
    "#list_data\n",
    " \n",
    "#pd.concat(list_data,axis = 1, sort = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
